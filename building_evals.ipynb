{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Evals\n",
    "Optimizing Claude to give you the highest possible accuracy on a task is an empirical science, and a process of continuous improvement. Whether you are trying to know if a change to your prompt made the model perform better on a key metric, or whether you are trying to gauge if the model is good enough to launch into production, a good system for offline evaluation is critical to success.\n",
    "\n",
    "In this recipe, we will walk through common patterns in building evaluations, and useful rules of thumb to follow when doing so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of an Eval\n",
    "Evals typically have four parts.\n",
    "- An input prompt that is fed to the model. We will ask Claude to generate a completion based on this prompt. Often when we design our evals the input column will contain a set of variable inputs that get fed into a prompt template at test time.\n",
    "- An output that comes from running the input prompt through the model we want to evaluate.\n",
    "- A \"golden answer\" to which we compare the model output. The golden answer could be a mandatory exact match, or it could be an example of a perfect answer meant to give a grader a point of comparison to base their scoring on.\n",
    "- A score, generated by one of the grading methods discussed below, that represents how the model did on the question.\n",
    "\n",
    "## Eval Grading Methods\n",
    "There are two things about evals that can be time consuming and expensive. The first is writing the questions and golden answers for the eval. The second is grading. Writing questions and golden answers can be quite time consuming if you do not have a dataset already available or a way to create one without manually generating questions (consider using Claude to generate your questions!), but has the benefit of typically being a one-time fixed cost. You write questions and golden answers, and very rarely have to re-write them. Grading on the other hand is a cost you will incur every time you re-run your eval, in perpetuity - and you will likely re-run your eval a lot. As a result, building evals that can be quickly and cheaply graded should be at the center of your design choices.\n",
    "\n",
    "There are three common ways to grade evals.\n",
    "- **Code-based grading:** This involves using standard code (mostly string matching and regular expressions) to grade the model's outputs. Common versions are checking for an exact match against an answer, or checking that a string contains some key phrase(s). This is by far the best grading method if you can design an eval that allows for it, as it is super fast and highly reliable. However, many evaluations do not allow for this style of grading.\n",
    "- **Human grading:** A human looks at the model-generated answer, compares it to the golden answer, and assigns a score. This is the most capable grading method as it _can_ be used on almost any task, but it is also incredibly slow and expensive, particularly if you've built a large eval. You should mostly try to avoid designing evals that require human grading if you can help it.\n",
    "- **Model-based grading:** It turns out that Claude is highly capable of grading itself, and can be used to grade a wide variety of tasks that might have historically required humans, such as analysis of tone in creative writing or accuracy in free-form question answering. You do this by writing a _grader prompt_ for Claude.\n",
    "\n",
    "Let's walk through an example of each grading method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code-based Grading\n",
    "Here we will be grading an eval where we ask Claude to successfully identify how many legs something has. We want Claude to output just a number of legs, and we design the eval in a way that we can use an exact-match code-based grader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and read in required packages, plus create an anthropic client.\n",
    "%pip install anthropic pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "client = Anthropic()\n",
    "MODEL_NAME = \"claude-3-opus-20240229\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our input prompt template for the task.\n",
    "def build_input_prompt(animal_statement):\n",
    "    user_content = f\"\"\"You will be provided a statement about an animal and your job is to determine how many legs that animal has.\n",
    "    \n",
    "    Here is the animal statment.\n",
    "    <animal_statement>{animal_statement}</animal_statment>\n",
    "    \n",
    "    How many legs does the animal have? Return just the number of legs as an integer and nothing else.\"\"\"\n",
    "\n",
    "    messages = [{'role': 'user', 'content': user_content}]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our eval (in practice you might do this as a jsonl or csv file instead).\n",
    "eval = [\n",
    "    {\n",
    "        \"animal_statement\": 'The animal is a human.',\n",
    "        \"golden_answer\": '2'\n",
    "    },\n",
    "        {\n",
    "        \"animal_statement\": 'The animal is a snake.',\n",
    "        \"golden_answer\": '0'\n",
    "    },\n",
    "        {\n",
    "        \"animal_statement\": 'The fox lost a leg, but then magically grew back the leg he lost and a mysterious extra leg on top of that.',\n",
    "        \"golden_answer\": '5'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animal Statement: The animal is a human.\n",
      "Golden Answer: 2\n",
      "Output: 2\n",
      "\n",
      "Animal Statement: The animal is a snake.\n",
      "Golden Answer: 0\n",
      "Output: 0\n",
      "\n",
      "Animal Statement: The fox lost a leg, but then magically grew back the leg he lost and a mysterious extra leg on top of that.\n",
      "Golden Answer: 5\n",
      "Output: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get completions for each input.\n",
    "# Define our get_completion function (including the stop sequence discussed above).\n",
    "def get_completion(messages):\n",
    "    response = client.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=5,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "# Get completions for each question in the eval.\n",
    "outputs = [get_completion(build_input_prompt(question['animal_statement'])) for question in eval]\n",
    "\n",
    "# Let's take a quick look at our outputs\n",
    "for output, question in zip(outputs, eval):\n",
    "    print(f\"Animal Statement: {question['animal_statement']}\\nGolden Answer: {question['golden_answer']}\\nOutput: {output}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Check our completions against the golden answers.\n",
    "# Define a grader function\n",
    "def grade_completion(output, golden_answer):\n",
    "    return output == golden_answer\n",
    "\n",
    "# Run the grader function on our outputs and print the score.\n",
    "grades = [grade_completion(output, question['golden_answer']) for output, question in zip(outputs, eval)]\n",
    "print(f\"Score: {sum(grades)/len(grades)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human grading\n",
    "Now let's imagine that we are grading an eval where we've asked Claude a series of open ended questions, maybe for a general purpose chat assistant. Unfortunately, answers could be varied and this can not be graded with code. One way we can do this is with human grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our input prompt template for the task.\n",
    "def build_input_prompt(question):\n",
    "    user_content = f\"\"\"Please answer the following question:\n",
    "    <question>{question}</question>\"\"\"\n",
    "\n",
    "    messages = [{'role': 'user', 'content': user_content}]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our eval. For this task, the best \"golden answer\" to give a human are instructions on what to look for in the model's output.\n",
    "eval = [\n",
    "    {\n",
    "        \"question\": 'Please design me a workout for today that features at least 50 reps of pulling leg exercises, at least 50 reps of pulling arm exercises, and ten minutes of core.',\n",
    "        \"golden_answer\": 'A correct answer should include a workout plan with 50 or more reps of pulling leg exercises (such as deadlifts, but not such as squats which are a pushing exercise), 50 or more reps of pulling arm exercises (such as rows, but not such as presses which are a pushing exercise), and ten minutes of core workouts. It can but does not have to include stretching or a dynamic warmup, but it cannot include any other meaningful exercises.'\n",
    "    },\n",
    "    {\n",
    "        \"question\": 'Send Jane an email asking her to meet me in front of the office at 9am to leave for the retreat.',\n",
    "        \"golden_answer\": 'A correct answer should decline to send the email since the assistant has no capabilities to send emails. It is okay to suggest a draft of the email, but not to attempt to send the email, call a function that sends the email, or ask for clarifying questions related to sending the email (such as which email address to send it to).'\n",
    "    },\n",
    "    {\n",
    "        \"question\": 'Who won the super bowl in 2024 and who did they beat?', # Claude should get this wrong since it comes after its training cutoff.\n",
    "        \"golden_answer\": 'A correct answer states that the Kansas City Chiefs defeated the San Francisco 49ers.'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Please design me a workout for today that features at least 50 reps of pulling leg exercises, at least 50 reps of pulling arm exercises, and ten minutes of core.\n",
      "Golden Answer: A correct answer should include a workout plan with 50 or more reps of pulling leg exercises (such as deadlifts, but not such as squats which are a pushing exercise), 50 or more reps of pulling arm exercises (such as rows, but not such as presses which are a pushing exercise), and ten minutes of core workouts. It can but does not have to include stretching or a dynamic warmup, but it cannot include any other meaningful exercises.\n",
      "Output: Here's a workout that meets your requirements:\n",
      "\n",
      "Leg Pulling Exercises (50 reps total):\n",
      "- Hamstring Curls: 3 sets of 10 reps (30 reps)\n",
      "- Single-leg Romanian Deadlifts: 2 sets of 10 reps per leg (20 reps)\n",
      "\n",
      "Arm Pulling Exercises (50 reps total):\n",
      "- Bent-over Rows: 3 sets of 10 reps (30 reps)\n",
      "- Pull-ups or Assisted Pull-ups: 2 sets of 10 reps (20 reps)\n",
      "\n",
      "Core Exercises (10 minutes):\n",
      "- Plank: 3 sets of 1 minute each\n",
      "- Russian Twists: 3 sets of 30 seconds each\n",
      "- Bicycle Crunches: 3 sets of 30 seconds each\n",
      "\n",
      "Workout Routine:\n",
      "1. Hamstring Curls: 3 sets of 10 reps, rest 30-60 seconds between sets\n",
      "2. Bent-over Rows: 3 sets of 10 reps, rest 30-60 seconds between sets\n",
      "3. Single-leg Romanian Deadlifts: 2 sets of 10 reps per leg, rest 30-60 seconds between sets\n",
      "4. Pull-ups or Assisted Pull-ups: 2 sets of 10 reps, rest 30-60 seconds between sets\n",
      "5. Core Circuit (repeat 3 times):\n",
      "   - Plank: 1 minute\n",
      "   - Russian Twists: 30 seconds\n",
      "   - Bicycle Crunches: 30 seconds\n",
      "   - Rest 30 seconds between each exercise\n",
      "\n",
      "Remember to warm up before starting the workout and cool down with some light stretching afterward. Adjust the weights, resistance, or number of reps according to your fitness level and comfort. Stay hydrated throughout the workout.\n",
      "\n",
      "Question: Send Jane an email asking her to meet me in front of the office at 9am to leave for the retreat.\n",
      "Golden Answer: A correct answer should decline to send the email since the assistant has no capabilities to send emails. It is okay to suggest a draft of the email, but not to attempt to send the email, call a function that sends the email, or ask for clarifying questions related to sending the email (such as which email address to send it to).\n",
      "Output: I apologize for the confusion, but I am not able to send emails on your behalf. As an AI language model, I don't have the capability to communicate directly with individuals or send emails.\n",
      "\n",
      "If you need to send an email to Jane, you will need to do so yourself using your own email account. In your email, you can include the necessary details, such as:\n",
      "\n",
      "\"Hi Jane,\n",
      "\n",
      "Could you please meet me in front of the office at 9am so we can leave together for the retreat?\n",
      "\n",
      "Thanks,\n",
      "[Your name]\"\n",
      "\n",
      "I hope this helps clarify the situation. Let me know if you have any other questions!\n",
      "\n",
      "Question: Who won the super bowl in 2024 and who did they beat?\n",
      "Golden Answer: A correct answer states that the Kansas City Chiefs defeated the San Francisco 49ers.\n",
      "Output: I apologize, but I cannot answer this question as it is based on a future event that has not happened yet. The Super Bowl for the 2024 season will be played in February 2025, and as of April 2023, the teams that will compete in that game are unknown. The participants and winner will be determined based on the results of the NFL regular season and playoffs leading up to the Super Bowl.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get completions for each input.\n",
    "# Define our get_completion function (including the stop sequence discussed above).\n",
    "def get_completion(messages):\n",
    "    response = client.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=2048,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "# Get completions for each question in the eval.\n",
    "outputs = [get_completion(build_input_prompt(question['question'])) for question in eval]\n",
    "\n",
    "# Let's take a quick look at our outputs\n",
    "for output, question in zip(outputs, eval):\n",
    "    print(f\"Question: {question['question']}\\nGolden Answer: {question['golden_answer']}\\nOutput: {output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we will need to have a human grade this question, from here you would evaluate the outputs against the golden answers yourself, or write the outputs and golden answers to a csv and hand them to another human grader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-based Grading\n",
    "Having to manually grade the above eval every time is going to get very annoying very fast, especially if the eval is a more realistic size (dozens, hundreds, or even thousands of questions). Luckily, there's a better way! We can actually have Claude do the grading for us. Let's take a look at how to do that using the same eval and completions from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 66.66666666666666%\n"
     ]
    }
   ],
   "source": [
    "# We start by defining a \"grader prompt\" template.\n",
    "def build_grader_prompt(answer, rubric):\n",
    "    user_content = f\"\"\"You will be provided an answer that an assistant gave to a question, and a rubric that instructs you on what makes the answer correct or incorrect.\n",
    "    \n",
    "    Here is the answer that the assistant gave to the question.\n",
    "    <answer>{answer}</answer>\n",
    "    \n",
    "    Here is the rubric on what makes the answer correct or incorrect.\n",
    "    <rubric>{rubric}</rubric>\n",
    "    \n",
    "    An answer is correct if it entirely meets the rubric criteria, and is otherwise incorrect. =\n",
    "    First, think through whether the answer is correct or incorrect based on the rubric inside <thinking></thinking> tags. Then, output either 'correct' if the answer is correct or 'incorrect' if the answer is incorrect inside <correctness></correctness> tags.\"\"\"\n",
    "\n",
    "    messages = [{'role': 'user', 'content': user_content}]\n",
    "    return messages\n",
    "\n",
    "# Now we define the full grade_completion function.\n",
    "import re\n",
    "def grade_completion(output, golden_answer):\n",
    "    messages = build_grader_prompt(output, golden_answer)\n",
    "    completion = get_completion(messages)\n",
    "    # Extract just the label from the completion (we don't care about the thinking)\n",
    "    pattern = r'<correctness>(.*?)</correctness>'\n",
    "    match = re.search(pattern, completion, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        raise ValueError(\"Did not find <correctness></correctness> tags.\")\n",
    "\n",
    "# Run the grader function on our outputs and print the score.\n",
    "grades = [grade_completion(output, question['golden_answer']) for output, question in zip(outputs, eval)]\n",
    "print(f\"Score: {grades.count('correct')/len(grades)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the claude-based grader is able to correctly analyze and grade Claude's responses with a high level of accuracy, saving you precious time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know about different grading design patterns for evals, and are ready to start building your own. As you do, here are a few guiding pieces of wisdom to get you started.\n",
    "- Make your evals specific to your task whenever possible, and try to have the distribution in your eval represent ~ the real life distribution of questions and question difficulties.\n",
    "- The only way to know if a model-based grader can do a good job grading your task is to try. Try it out and read some samples to see if your task is a good candidate.\n",
    "- Often all that lies between you and an automatable eval is clever design. Try to structure questions in a way that the grading can be automated, while still staying true to the task. Reformatting questions into multiple choice is a common tactic here.\n",
    "- In general, your preference should be for higher volume and lower quality of questions over very low volume with high quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Grading for [ReallyEasy.AI Prompt Engineering Session](https://www.youtube.com/watch?v=jlkBdje_wLI)\n",
    "\n",
    "Now we apply the model grading to our scenario. Recall that our goal is to do the following:\n",
    "- Take AI research papers as input\n",
    "- Summarize the papers in the following way:\n",
    "    - Set the reading level to that of a high school student\n",
    "    - Give a one paragraph summary\n",
    "    - Give 3-5 key points of the paper\n",
    "    - Give 3-5 key quotes from the paper\n",
    "\n",
    "Using this rubric (i.e. clear criteria) we will have the model look at each paper in the papers folder and  grade the output for us while we also view the output and judge for ourselves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper: Addressing Social Misattributions of Large Language Models.pdf\n",
      "Summary:\n",
      "Summary:\n",
      "This academic paper discusses the potential risks of social misattributions in Large Language Models (LLMs) and proposes a solution using the Social Transparency framework. LLMs are AI systems capable of simulating various roles and personas in conversations with users. However, there might be a mismatch between the roles and personas intended by the designers and those perceived by the users. This can lead to unwarranted trust, emotional manipulation, and dangerous behaviors. To address this issue, the authors suggest extending the Social Transparency framework by adding a fifth 'W-question' to clarify the specific social attributions assigned to LLMs.\n",
      "\n",
      "Key Points:\n",
      "1. LLMs can simulate various roles and personas in conversations with users.\n",
      "2. There is a risk of social misattributions, where users' perceptions of LLM roles and personas differ from designers' intentions.\n",
      "3. Social misattributions can lead to unwarranted trust, emotional manipulation, and dangerous behaviors.\n",
      "4. The authors propose extending the Social Transparency framework with a fifth 'W-question' to clarify social attributions.\n",
      "5. Developing taxonomies of social attributions and implementing techniques to detect and prevent misattributions are suggested solutions.\n",
      "\n",
      "Key Quotes:\n",
      "1. \"LLMs, which are remarkably capable of simulating roles and personas, may lead to mismatches between designers' intentions and users' perceptions of social attributes, risking to promote emotional manipulation and dangerous behaviors, cases of epistemic injustice, and unwarranted trust.\"\n",
      "2. \"Research has shown that ChatGPT-3.5 prescribed medications to individuals affected by anxiety or depression, despite this being not allowed to such systems.\"\n",
      "3. \"Social misattributions of LLMs lead to unwarranted trust in these systems.\"\n",
      "4. \"To address the problem of social misattribution of LLMs, our proposal is to adapt the Social Transparency framework by augmenting the '4W' model to a '5W' model.\"\n",
      "5. \"Organizations that develop LLM-based applications or use them for their services and products should provide taxonomies of the appropriate and inappropriate roles and personas for their systems.\"\n",
      "Grade: Correct\n",
      "\n",
      "The summary, key points, and quotes meet all the specified criteria:\n",
      "\n",
      "1. Understandable by a high school student: The language used is clear and accessible, making it easy for a high school student to comprehend the main ideas and concepts.\n",
      "\n",
      "2. Concise, one-paragraph summary: The summary is presented in a single, well-structured paragraph that effectively conveys the main points of the academic paper.\n",
      "\n",
      "3. Includes 3-5 clear key points: The key points section lists five distinct and relevant points that capture the essential aspects of the paper, including the potential risks of social misattributions in LLMs and the proposed solution.\n",
      "\n",
      "4. Contains 3-5 relevant quotes: The provided quotes are directly relevant to the main ideas discussed in the paper, and they effectively support the key points and summary. The quotes are well-chosen and provide valuable insights into the research findings and proposals.\n",
      "\n",
      "Paper: AgentStudio - A Toolkit for Building General Virtual Agents.pdf\n",
      "Summary:\n",
      "Summary:\n",
      "AgentStudio is a new toolkit designed to help researchers create and test AI agents that can use any software on a computer. It provides a realistic online environment where agents can be developed and evaluated on various tasks. The toolkit includes features for data collection, agent testing, and result visualization.\n",
      "\n",
      "Key Points:\n",
      "1. AgentStudio offers a generic observation and action space, allowing agents to interact with software through both human-like actions (e.g. keyboard and mouse) and code.\n",
      "2. The online, realistic environments in AgentStudio are compatible with different operating systems and devices, enabling agent development and testing in real-world settings.\n",
      "3. AgentStudio includes user-friendly interfaces for collecting datasets of human demonstrations and agent trajectories, as well as for monitoring agent behaviors.\n",
      "4. The toolkit introduces a GUI grounding dataset and a real-world benchmark suite to evaluate fundamental agent abilities like accurate GUI interaction and long-horizon planning.\n",
      "5. AgentStudio highlights several promising research directions, such as visual grounding, learning from videos, open-ended tool creation, and developing a generalist critic model.\n",
      "\n",
      "Key Quotes:\n",
      "1. \"AgentStudio is a holistic platform built on real-world environments to facilitate agent development. It extends beyond a typical benchmark with the following key features:\"\n",
      "2. \"The generic observation and action spaces consist of both human-computer interfaces and function calling. Specifically, agents interact with external environments through an interactive Python kernel.\"\n",
      "3. \"AgentStudio provides unified observation and action spaces that is compatible with both how humans interact with computers and function calling. This feature enables agent evaluation and data collection on any human-performed task, which drastically expands the potential task space.\"\n",
      "4. \"To illustrate how AgentStudio facilitates agent evaluation in complex and real-world scenarios, we introduce a benchmark suite consisting of 77 real-world tasks. Despite being conceptually simple, these tasks pose challenges for current state-of-the-art virtual agents.\"\n",
      "5. \"AgentStudio has the potential to promote several lines of research that are essential for building general virtual agents. Here, we offer several possible research problems that are emphasized in the design choices of AgentStudio or encountered during its development.\"\n",
      "Grade: Correct\n",
      "\n",
      "The summary, key points, and quotes meet all the specified criteria:\n",
      "\n",
      "1. The language used is clear and straightforward, making it understandable for a high school student.\n",
      "\n",
      "2. The summary is concise and captures the main aspects of AgentStudio in a single paragraph.\n",
      "\n",
      "3. The key points section includes 5 clear and relevant points that highlight the main features and benefits of AgentStudio.\n",
      "\n",
      "4. The quotes section contains 5 relevant quotes that support the summary and key points, providing additional context and detail.\n",
      "\n",
      "Overall, the provided summary, key points, and quotes effectively convey the essential information about AgentStudio in a clear, concise, and well-structured manner.\n",
      "\n",
      "Paper: AI Safety - Necessary but insufficient and possibly problematic.pdf\n",
      "Summary:\n",
      "Summary (high school reading level):\n",
      "\n",
      "This article questions the recent focus on AI safety by governments and big tech companies. The author argues that while AI safety is important, the current discussions around it are not enough to address the potential harms AI can cause to society. The article points out that the AI research community has been working on making AI more ethical, fair, and accountable for years, but the current AI safety discussions don't seem to align with these goals. The author is concerned that the focus on AI safety may actually allow harmful AI systems to continue operating under the guise of being \"safe.\"\n",
      "\n",
      "Key Points:\n",
      "1. The recent focus on AI safety by governments and corporations differs from the AI research community's work on AI ethics, fairness, and accountability.\n",
      "2. AI safety discussions don't seem to prioritize transparency, which is crucial for identifying and addressing potential harms caused by AI.\n",
      "3. The current notion of AI safety may not be sufficient to prevent AI from causing structural harm to society.\n",
      "4. The AI safety debate has already influenced some AI regulations, possibly in undesirable ways.\n",
      "5. The focus on AI safety might allow harmful AI systems to continue operating under the label of being \"safe.\"\n",
      "\n",
      "Key Quotes:\n",
      "1. \"AI safety has a nuanced and uneasy relationship with transparency and other allied notions associated with societal good, indicating that it is an insufficient notion if the goal is that of societal good in a broad sense.\"\n",
      "2. \"If AI safety is primed towards addressing visible harm, this begs the question as to where AI safety stands with respect to mandating visibility in AI.\"\n",
      "3. \"If indeed the shallow notion of AI safety that allows for continuation of structural harm takes on the front seat in debates about the future of AI, that may lead to many undesirable consequences.\"\n",
      "4. \"AI that is intentionally harmful – such as racist predictive policing algorithms, discriminatory insurance premium determination AI, poor and unsustainable wages determined by gig AI optimized for platform profits – could be fomented under the label of 'safe AI'.\"\n",
      "5. \"The contours of the emerging AI safety debate is too risky for humanity in that it may normalize the most potent negative consequence from AI, that of structural harm, and secondly, opaqueness of AI.\"\n",
      "Grade: correct\n",
      "\n",
      "The summary meets all the specified criteria:\n",
      "\n",
      "1. It is written in clear, simple language that a high school student can understand.\n",
      "\n",
      "2. The summary is concise and presented in a single paragraph.\n",
      "\n",
      "3. It includes 5 clear key points that capture the main ideas of the article.\n",
      "\n",
      "4. The summary contains 5 relevant quotes that support the key points and provide insight into the author's perspective.\n",
      "\n",
      "Overall, the summary effectively condenses the article's content, highlights the main arguments, and includes relevant supporting quotes, making it a well-structured and informative summary suitable for a high school reading level.\n",
      "\n",
      "Paper: Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models.pdf\n",
      "Summary:\n",
      "Sure, here is a summary of the academic paper \"Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models Versus Fine-Tuned Vision Transformers in Image-Based Security Applications\" at a high school reading level:\n",
      "\n",
      "Paragraph Summary:\n",
      "This study compares how well two different types of artificial intelligence models - large multimodal models engineered with prompts and fine-tuned vision transformer models - perform on two cybersecurity tasks involving analyzing images. The researchers found that the fine-tuned vision transformer models did much better than the prompt-engineered large multimodal models on both detecting hidden triggers in images that could indicate a security threat and classifying what type of malware an image represented. \n",
      "\n",
      "Key Points:\n",
      "1. The study tested the AI models on two tasks: detecting small hidden squares in images that could be security triggers, and classifying images of malware into different types.\n",
      "\n",
      "2. The fine-tuned vision transformer models achieved nearly perfect accuracy on both tasks. \n",
      "\n",
      "3. The prompt-engineered large multimodal models struggled, especially with classifying the malware images, even when given very specific prompts and example images.\n",
      "\n",
      "4. The results suggest fine-tuned vision transformers are better suited for these types of visual cybersecurity tasks than large multimodal models, even with prompt engineering.\n",
      "\n",
      "Key Quotes:\n",
      "1. \"This study aims to evaluate the efficacy of prompt-engineered LMMs, such as Gemini-pro models, compared to fine-tuned Vision Transformer (ViT) models in the cybersecurity domain.\"\n",
      "\n",
      "2. \"For trigger detection, the fine-tuned ViT models showcased remarkable proficiency, achieving 100% accuracy in identifying adversarial triggers.\"\n",
      "\n",
      "3. \"For malware classification, the fine-tuned ViT models demonstrated superior performance, with accuracy rates of 97.12% for classifying among 25 malware types and 98.00% for classifying among 5 malware families.\"\n",
      "\n",
      "4. \"The findings suggest that while prompt engineering can enhance the utility of LMMs to some extent, the approach may not always be sufficient for tasks requiring high-level visual comprehension or pattern recognition.\"\n",
      "\n",
      "5. \"The superior performance of ViTs in both visually evident and non-visually evident tasks highlights their potential as a reliable and effective solution for diverse cybersecurity challenges.\"\n",
      "Grade: The summary meets all the specified criteria:\n",
      "\n",
      "1. Understandable by a high school student: The language used in the summary, key points, and quotes is clear and accessible to a high school-level reader.\n",
      "\n",
      "2. Concise, one-paragraph summary: The summary is concise and effectively conveys the main findings of the study in a single paragraph.\n",
      "\n",
      "3. Includes 3-5 clear key points: The summary includes four key points that highlight the main aspects of the study and its findings.\n",
      "\n",
      "4. Contains 3-5 relevant quotes: The summary includes five relevant quotes from the original paper that support the key points and provide additional context.\n",
      "\n",
      "Grade: Correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from anthropic import Anthropic\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# Putting everything in one place for easy use elsewhere.\n",
    "# Most of the functions here are the same as the function in the Anthropic official examples, but with a different name to avoid conflicts.\n",
    "\n",
    "MODEL_NAME = \"claude-3-opus-20240229\"  \n",
    "client = Anthropic() # assumes you have an API key in your environment variables called ANTHROPIC_API_KEY\n",
    "\n",
    "def reallyeasyai_get_completion(client, model_name, prompt):\n",
    "    response = client.messages.create(\n",
    "        model=model_name,\n",
    "        max_tokens=2048,\n",
    "        messages=[{\"role\": 'user', \"content\":  prompt}]\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "\n",
    "\n",
    "def reallyeasyai_read_pdf(file_path):\n",
    "    \"\"\"Reads text from a PDF file and concatenates it into a single string.\"\"\"\n",
    "    reader = PdfReader(file_path)\n",
    "    text = ''.join([page.extract_text() for page in reader.pages if page.extract_text() is not None])\n",
    "    return text\n",
    "\n",
    "def reallyeasyai_generate_summary(client, paper_text):\n",
    "    \"\"\"Generates a summary for the given paper text.\"\"\"\n",
    "    summary_prompt = f\"\"\"\n",
    "    Here is an academic paper: <paper>{paper_text}</paper>\n",
    "    Please summarize this paper at a high school reading level, providing a paragraph summary, 3-5 key points, and 3-5 key quotes.\n",
    "    \"\"\"\n",
    "    return reallyeasyai_get_completion(client, MODEL_NAME, summary_prompt)\n",
    "\n",
    "def reallyeasyai_grade_summary(client, summary):\n",
    "    \"\"\"Grades the given summary based on predefined criteria.\"\"\"\n",
    "    grading_prompt = f\"\"\"\n",
    "    Given this summary, key points, and quotes: {summary}\n",
    "    Assess whether it meets the following criteria:\n",
    "    - Understandable by a high school student.\n",
    "    - Concise, one-paragraph summary.\n",
    "    - Includes 3-5 clear key points.\n",
    "    - Contains 3-5 relevant quotes.\n",
    "    Grade as 'correct' if all criteria are met, otherwise 'needs improvement'.\n",
    "    \"\"\"\n",
    "    return reallyeasyai_get_completion(client, MODEL_NAME, grading_prompt)\n",
    "\n",
    "def reallyeasyai_process_paper(client, file_path):\n",
    "    \"\"\"Processes each paper by generating a summary and grading it.\"\"\"\n",
    "    paper_text = reallyeasyai_read_pdf(file_path)\n",
    "    summary = reallyeasyai_generate_summary(client, paper_text)\n",
    "    grade = reallyeasyai_grade_summary(client, summary)\n",
    "\n",
    "    print(f\"Paper: {os.path.basename(file_path)}\\nSummary:\\n{summary}\\nGrade: {grade}\\n\")\n",
    "\n",
    "# Path to your papers folder and the AI model client initialization\n",
    "papers_folder = './papers'\n",
    "# Ensure the 'client' is properly initialized according to your environment\n",
    "\n",
    "for file_name in os.listdir(papers_folder):\n",
    "    if file_name.endswith('.pdf'):\n",
    "        file_path = os.path.join(papers_folder, file_name)\n",
    "        reallyeasyai_process_paper(client, file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
