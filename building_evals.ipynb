{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Evals\n",
    "Optimizing Claude to give you the highest possible accuracy on a task is an empirical science, and a process of continuous improvement. Whether you are trying to know if a change to your prompt made the model perform better on a key metric, or whether you are trying to gauge if the model is good enough to launch into production, a good system for offline evaluation is critical to success.\n",
    "\n",
    "In this recipe, we will walk through common patterns in building evaluations, and useful rules of thumb to follow when doing so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of an Eval\n",
    "Evals typically have four parts.\n",
    "- An input prompt that is fed to the model. We will ask Claude to generate a completion based on this prompt. Often when we design our evals the input column will contain a set of variable inputs that get fed into a prompt template at test time.\n",
    "- An output that comes from running the input prompt through the model we want to evaluate.\n",
    "- A \"golden answer\" to which we compare the model output. The golden answer could be a mandatory exact match, or it could be an example of a perfect answer meant to give a grader a point of comparison to base their scoring on.\n",
    "- A score, generated by one of the grading methods discussed below, that represents how the model did on the question.\n",
    "\n",
    "## Eval Grading Methods\n",
    "There are two things about evals that can be time consuming and expensive. The first is writing the questions and golden answers for the eval. The second is grading. Writing questions and golden answers can be quite time consuming if you do not have a dataset already available or a way to create one without manually generating questions (consider using Claude to generate your questions!), but has the benefit of typically being a one-time fixed cost. You write questions and golden answers, and very rarely have to re-write them. Grading on the other hand is a cost you will incur every time you re-run your eval, in perpetuity - and you will likely re-run your eval a lot. As a result, building evals that can be quickly and cheaply graded should be at the center of your design choices.\n",
    "\n",
    "There are three common ways to grade evals.\n",
    "- **Code-based grading:** This involves using standard code (mostly string matching and regular expressions) to grade the model's outputs. Common versions are checking for an exact match against an answer, or checking that a string contains some key phrase(s). This is by far the best grading method if you can design an eval that allows for it, as it is super fast and highly reliable. However, many evaluations do not allow for this style of grading.\n",
    "- **Human grading:** A human looks at the model-generated answer, compares it to the golden answer, and assigns a score. This is the most capable grading method as it _can_ be used on almost any task, but it is also incredibly slow and expensive, particularly if you've built a large eval. You should mostly try to avoid designing evals that require human grading if you can help it.\n",
    "- **Model-based grading:** It turns out that Claude is highly capable of grading itself, and can be used to grade a wide variety of tasks that might have historically required humans, such as analysis of tone in creative writing or accuracy in free-form question answering. You do this by writing a _grader prompt_ for Claude.\n",
    "\n",
    "Let's walk through an example of each grading method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code-based Grading\n",
    "Here we will be grading an eval where we ask Claude to successfully identify how many legs something has. We want Claude to output just a number of legs, and we design the eval in a way that we can use an exact-match code-based grader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and read in required packages, plus create an anthropic client.\n",
    "%pip install anthropic pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "client = Anthropic()\n",
    "MODEL_NAME = \"claude-3-opus-20240229\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our input prompt template for the task.\n",
    "def build_input_prompt(animal_statement):\n",
    "    user_content = f\"\"\"You will be provided a statement about an animal and your job is to determine how many legs that animal has.\n",
    "    \n",
    "    Here is the animal statment.\n",
    "    <animal_statement>{animal_statement}</animal_statment>\n",
    "    \n",
    "    How many legs does the animal have? Return just the number of legs as an integer and nothing else.\"\"\"\n",
    "\n",
    "    messages = [{'role': 'user', 'content': user_content}]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our eval (in practice you might do this as a jsonl or csv file instead).\n",
    "eval = [\n",
    "    {\n",
    "        \"animal_statement\": 'The animal is a human.',\n",
    "        \"golden_answer\": '2'\n",
    "    },\n",
    "        {\n",
    "        \"animal_statement\": 'The animal is a snake.',\n",
    "        \"golden_answer\": '0'\n",
    "    },\n",
    "        {\n",
    "        \"animal_statement\": 'The fox lost a leg, but then magically grew back the leg he lost and a mysterious extra leg on top of that.',\n",
    "        \"golden_answer\": '5'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animal Statement: The animal is a human.\n",
      "Golden Answer: 2\n",
      "Output: 2\n",
      "\n",
      "Animal Statement: The animal is a snake.\n",
      "Golden Answer: 0\n",
      "Output: 0\n",
      "\n",
      "Animal Statement: The fox lost a leg, but then magically grew back the leg he lost and a mysterious extra leg on top of that.\n",
      "Golden Answer: 5\n",
      "Output: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get completions for each input.\n",
    "# Define our get_completion function (including the stop sequence discussed above).\n",
    "def get_completion(messages):\n",
    "    response = client.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=5,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "# Get completions for each question in the eval.\n",
    "outputs = [get_completion(build_input_prompt(question['animal_statement'])) for question in eval]\n",
    "\n",
    "# Let's take a quick look at our outputs\n",
    "for output, question in zip(outputs, eval):\n",
    "    print(f\"Animal Statement: {question['animal_statement']}\\nGolden Answer: {question['golden_answer']}\\nOutput: {output}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Check our completions against the golden answers.\n",
    "# Define a grader function\n",
    "def grade_completion(output, golden_answer):\n",
    "    return output == golden_answer\n",
    "\n",
    "# Run the grader function on our outputs and print the score.\n",
    "grades = [grade_completion(output, question['golden_answer']) for output, question in zip(outputs, eval)]\n",
    "print(f\"Score: {sum(grades)/len(grades)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human grading\n",
    "Now let's imagine that we are grading an eval where we've asked Claude a series of open ended questions, maybe for a general purpose chat assistant. Unfortunately, answers could be varied and this can not be graded with code. One way we can do this is with human grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our input prompt template for the task.\n",
    "def build_input_prompt(question):\n",
    "    user_content = f\"\"\"Please answer the following question:\n",
    "    <question>{question}</question>\"\"\"\n",
    "\n",
    "    messages = [{'role': 'user', 'content': user_content}]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our eval. For this task, the best \"golden answer\" to give a human are instructions on what to look for in the model's output.\n",
    "eval = [\n",
    "    {\n",
    "        \"question\": 'Please design me a workout for today that features at least 50 reps of pulling leg exercises, at least 50 reps of pulling arm exercises, and ten minutes of core.',\n",
    "        \"golden_answer\": 'A correct answer should include a workout plan with 50 or more reps of pulling leg exercises (such as deadlifts, but not such as squats which are a pushing exercise), 50 or more reps of pulling arm exercises (such as rows, but not such as presses which are a pushing exercise), and ten minutes of core workouts. It can but does not have to include stretching or a dynamic warmup, but it cannot include any other meaningful exercises.'\n",
    "    },\n",
    "    {\n",
    "        \"question\": 'Send Jane an email asking her to meet me in front of the office at 9am to leave for the retreat.',\n",
    "        \"golden_answer\": 'A correct answer should decline to send the email since the assistant has no capabilities to send emails. It is okay to suggest a draft of the email, but not to attempt to send the email, call a function that sends the email, or ask for clarifying questions related to sending the email (such as which email address to send it to).'\n",
    "    },\n",
    "    {\n",
    "        \"question\": 'Who won the super bowl in 2024 and who did they beat?', # Claude should get this wrong since it comes after its training cutoff.\n",
    "        \"golden_answer\": 'A correct answer states that the Kansas City Chiefs defeated the San Francisco 49ers.'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Please design me a workout for today that features at least 50 reps of pulling leg exercises, at least 50 reps of pulling arm exercises, and ten minutes of core.\n",
      "Golden Answer: A correct answer should include a workout plan with 50 or more reps of pulling leg exercises (such as deadlifts, but not such as squats which are a pushing exercise), 50 or more reps of pulling arm exercises (such as rows, but not such as presses which are a pushing exercise), and ten minutes of core workouts. It can but does not have to include stretching or a dynamic warmup, but it cannot include any other meaningful exercises.\n",
      "Output: Here's a workout that meets your requirements:\n",
      "\n",
      "Leg Pulling Exercises (50 reps total):\n",
      "- Hamstring Curls: 3 sets of 10 reps (30 reps)\n",
      "- Single-leg Romanian Deadlifts: 2 sets of 10 reps per leg (20 reps)\n",
      "\n",
      "Arm Pulling Exercises (50 reps total):\n",
      "- Bent-over Rows: 3 sets of 10 reps (30 reps)\n",
      "- Pull-ups or Assisted Pull-ups: 2 sets of 10 reps (20 reps)\n",
      "\n",
      "Core Exercises (10 minutes):\n",
      "- Plank: 3 sets of 1 minute each\n",
      "- Russian Twists: 3 sets of 30 seconds each\n",
      "- Bicycle Crunches: 3 sets of 30 seconds each\n",
      "\n",
      "Workout Routine:\n",
      "1. Hamstring Curls: 3 sets of 10 reps, rest 30-60 seconds between sets\n",
      "2. Bent-over Rows: 3 sets of 10 reps, rest 30-60 seconds between sets\n",
      "3. Single-leg Romanian Deadlifts: 2 sets of 10 reps per leg, rest 30-60 seconds between sets\n",
      "4. Pull-ups or Assisted Pull-ups: 2 sets of 10 reps, rest 30-60 seconds between sets\n",
      "5. Core Circuit (repeat 3 times):\n",
      "   - Plank: 1 minute\n",
      "   - Russian Twists: 30 seconds\n",
      "   - Bicycle Crunches: 30 seconds\n",
      "   - Rest 30 seconds between each exercise\n",
      "\n",
      "Remember to warm up before starting the workout and cool down with some light stretching afterward. Adjust the weights, resistance, or number of reps according to your fitness level and comfort. Stay hydrated throughout the workout.\n",
      "\n",
      "Question: Send Jane an email asking her to meet me in front of the office at 9am to leave for the retreat.\n",
      "Golden Answer: A correct answer should decline to send the email since the assistant has no capabilities to send emails. It is okay to suggest a draft of the email, but not to attempt to send the email, call a function that sends the email, or ask for clarifying questions related to sending the email (such as which email address to send it to).\n",
      "Output: I apologize for the confusion, but I am not able to send emails on your behalf. As an AI language model, I don't have the capability to communicate directly with individuals or send emails.\n",
      "\n",
      "If you need to send an email to Jane, you will need to do so yourself using your own email account. In your email, you can include the necessary details, such as:\n",
      "\n",
      "\"Hi Jane,\n",
      "\n",
      "Could you please meet me in front of the office at 9am so we can leave together for the retreat?\n",
      "\n",
      "Thanks,\n",
      "[Your name]\"\n",
      "\n",
      "I hope this helps clarify the situation. Let me know if you have any other questions!\n",
      "\n",
      "Question: Who won the super bowl in 2024 and who did they beat?\n",
      "Golden Answer: A correct answer states that the Kansas City Chiefs defeated the San Francisco 49ers.\n",
      "Output: I apologize, but I cannot answer this question as it is based on a future event that has not happened yet. The Super Bowl for the 2024 season will be played in February 2025, and as of April 2023, the teams that will compete in that game are unknown. The participants and winner will be determined based on the results of the NFL regular season and playoffs leading up to the Super Bowl.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get completions for each input.\n",
    "# Define our get_completion function (including the stop sequence discussed above).\n",
    "def get_completion(messages):\n",
    "    response = client.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=2048,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "# Get completions for each question in the eval.\n",
    "outputs = [get_completion(build_input_prompt(question['question'])) for question in eval]\n",
    "\n",
    "# Let's take a quick look at our outputs\n",
    "for output, question in zip(outputs, eval):\n",
    "    print(f\"Question: {question['question']}\\nGolden Answer: {question['golden_answer']}\\nOutput: {output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we will need to have a human grade this question, from here you would evaluate the outputs against the golden answers yourself, or write the outputs and golden answers to a csv and hand them to another human grader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-based Grading\n",
    "Having to manually grade the above eval every time is going to get very annoying very fast, especially if the eval is a more realistic size (dozens, hundreds, or even thousands of questions). Luckily, there's a better way! We can actually have Claude do the grading for us. Let's take a look at how to do that using the same eval and completions from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 66.66666666666666%\n"
     ]
    }
   ],
   "source": [
    "# We start by defining a \"grader prompt\" template.\n",
    "def build_grader_prompt(answer, rubric):\n",
    "    user_content = f\"\"\"You will be provided an answer that an assistant gave to a question, and a rubric that instructs you on what makes the answer correct or incorrect.\n",
    "    \n",
    "    Here is the answer that the assistant gave to the question.\n",
    "    <answer>{answer}</answer>\n",
    "    \n",
    "    Here is the rubric on what makes the answer correct or incorrect.\n",
    "    <rubric>{rubric}</rubric>\n",
    "    \n",
    "    An answer is correct if it entirely meets the rubric criteria, and is otherwise incorrect. =\n",
    "    First, think through whether the answer is correct or incorrect based on the rubric inside <thinking></thinking> tags. Then, output either 'correct' if the answer is correct or 'incorrect' if the answer is incorrect inside <correctness></correctness> tags.\"\"\"\n",
    "\n",
    "    messages = [{'role': 'user', 'content': user_content}]\n",
    "    return messages\n",
    "\n",
    "# Now we define the full grade_completion function.\n",
    "import re\n",
    "def grade_completion(output, golden_answer):\n",
    "    messages = build_grader_prompt(output, golden_answer)\n",
    "    completion = get_completion(messages)\n",
    "    # Extract just the label from the completion (we don't care about the thinking)\n",
    "    pattern = r'<correctness>(.*?)</correctness>'\n",
    "    match = re.search(pattern, completion, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        raise ValueError(\"Did not find <correctness></correctness> tags.\")\n",
    "\n",
    "# Run the grader function on our outputs and print the score.\n",
    "grades = [grade_completion(output, question['golden_answer']) for output, question in zip(outputs, eval)]\n",
    "print(f\"Score: {grades.count('correct')/len(grades)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the claude-based grader is able to correctly analyze and grade Claude's responses with a high level of accuracy, saving you precious time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know about different grading design patterns for evals, and are ready to start building your own. As you do, here are a few guiding pieces of wisdom to get you started.\n",
    "- Make your evals specific to your task whenever possible, and try to have the distribution in your eval represent ~ the real life distribution of questions and question difficulties.\n",
    "- The only way to know if a model-based grader can do a good job grading your task is to try. Try it out and read some samples to see if your task is a good candidate.\n",
    "- Often all that lies between you and an automatable eval is clever design. Try to structure questions in a way that the grading can be automated, while still staying true to the task. Reformatting questions into multiple choice is a common tactic here.\n",
    "- In general, your preference should be for higher volume and lower quality of questions over very low volume with high quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Grading for [ReallyEasy.AI Prompt Engineering Session](https://www.youtube.com/watch?v=jlkBdje_wLI)\n",
    "\n",
    "Now we apply the model grading to our scenario. Recall that our goal is to do the following:\n",
    "- Take AI research papers as input\n",
    "- Summarize the papers in the following way:\n",
    "    - Set the reading level to that of a high school student\n",
    "    - Give a one paragraph summary\n",
    "    - Give 3-5 key points of the paper\n",
    "    - Give 3-5 key quotes from the paper\n",
    "\n",
    "Using this rubric (i.e. clear criteria) we will have the model look at each paper in the papers folder and  grade the output for us while we also view the output and judge for ourselves.\n",
    "\n",
    "NOTE: The code below is meant to be completely self-contained so you can copy and paste the two cells in any Jupyter notebook and they should work with little to no adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and read in required packages\n",
    "%pip install anthropic pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "\n",
      "Paper: Addressing Social Misattributions of Large Language Models.pdf\n",
      "\n",
      "=====\n",
      "Response from the Model:\n",
      "Paragraph Summary:\n",
      "This academic paper discusses the potential risks of social misattributions in Large Language Models (LLMs) and proposes a solution using the Human-Centered Explainable AI (HCXAI) approach. The authors argue that users may assign inappropriate roles and personas to LLMs, leading to unwarranted trust and potential harm, especially in sensitive areas like mental health. To address this issue, they suggest extending the existing Social Transparency framework by adding a fifth 'W-question' to clarify the specific social attributions assigned to LLMs by designers and users.\n",
      "\n",
      "Key Points:\n",
      "1. LLMs are capable of simulating various roles and personas, which may lead to mismatches between designers' intentions and users' perceptions.\n",
      "2. Social misattributions of LLMs can result in unwarranted trust, emotional manipulation, and dangerous behaviors, particularly in sensitive domains like mental health.\n",
      "3. The authors propose adapting the Social Transparency framework by adding a fifth 'W-question' to address the issue of social misattributions in LLMs.\n",
      "4. Developing a taxonomy of social attributions and implementing techniques to detect and prevent social misattributions are suggested as methods to support the integration of the fifth 'W-question'.\n",
      "\n",
      "Key Quotes:\n",
      "1. \"LLMs, which are remarkably capable of simulating roles and personas, may lead to mismatches between designers' intentions and users' perceptions of social attributes, risking to promote emotional manipulation and dangerous behaviors, cases of epistemic injustice, and unwarranted trust.\"\n",
      "2. \"Social attributions have a normative component: an LLM endowed with a given role and persona is expected to perform as such.\"\n",
      "3. \"Social misattributions of LLMs lead to unwarranted trust in these systems.\"\n",
      "4. \"To address the problem of social misattribution of LLMs, our proposal is to adapt the Social Transparency framework by augmenting the '4W' model to a '5W' model. The additional 'W-question' focuses on identifying (1) which social attributions are justified for a LLM in a certain context, and (2) which social attributions a user assigns to the LLM in the same context.\"\n",
      "=====\n",
      "Grade:\n",
      "The summary meets all the specified criteria:\n",
      "\n",
      "1. Understandable by a high school student: The language used in the summary is clear and accessible, making it easy for a high school student to understand the main ideas and concepts.\n",
      "\n",
      "2. Concise, one-paragraph summary: The summary is presented in a single, well-structured paragraph that effectively conveys the key information from the academic paper.\n",
      "\n",
      "3. Includes 3-5 clear key points: The summary includes four key points that highlight the main arguments and proposed solution discussed in the paper.\n",
      "\n",
      "4. Contains 3-5 relevant quotes: The summary includes four relevant quotes from the original text that support and illustrate the key points.\n",
      "\n",
      "Grade: Correct\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "Paper: AgentStudio - A Toolkit for Building General Virtual Agents.pdf\n",
      "\n",
      "=====\n",
      "Response from the Model:\n",
      "Summary:\n",
      "AgentStudio is a new platform that helps researchers create and test AI agents that can use any software on any computer. It provides a realistic online environment where agents can interact with the computer like humans do, using the mouse, keyboard, and code. AgentStudio also includes tools for collecting data, evaluating the agents, and visualizing their performance. The researchers used AgentStudio to create a dataset for testing how well agents can click on the right parts of the screen, and a set of tasks to test the agents' overall abilities.\n",
      "\n",
      "Key Points:\n",
      "1. AgentStudio is an online platform that enables the development and testing of AI agents that can control any software on any computer.\n",
      "2. It provides a realistic environment where agents can interact using the mouse, keyboard, and code, just like humans do.\n",
      "3. AgentStudio includes tools for data collection, agent evaluation, and performance visualization.\n",
      "4. The researchers used AgentStudio to create a dataset for testing agents' ability to click on the correct parts of the screen.\n",
      "5. They also developed a set of tasks to evaluate the agents' overall performance in real-world scenarios.\n",
      "\n",
      "Key Quotes:\n",
      "1. \"AgentStudio is a toolkit encompassing the entire lifecycle of building and benchmarking general virtual agents in the wild.\"\n",
      "2. \"The generic observation and action spaces consist of both human-computer interfaces and function calling.\"\n",
      "3. \"AgentStudio provides an integrated solution spanning environment setup, data collection, online testing, and result visualization.\"\n",
      "4. \"We utilize the AgentStudio interface to create a GUI grounding dataset across various applications and operating systems.\"\n",
      "5. \"We introduce an real-world task suite that ranges from simple function calling to complex cross-application tasks.\"\n",
      "=====\n",
      "Grade:\n",
      "Correct\n",
      "\n",
      "The summary of AgentStudio meets all the specified criteria:\n",
      "\n",
      "1. Understandable by a high school student: The summary is written in clear, simple language that a high school student should be able to understand.\n",
      "\n",
      "2. Concise, one-paragraph summary: The summary is presented in a single, concise paragraph that captures the essential information about AgentStudio.\n",
      "\n",
      "3. Includes 3-5 clear key points: The summary is followed by five key points that highlight the main features and applications of AgentStudio.\n",
      "\n",
      "4. Contains 3-5 relevant quotes: There are five relevant quotes provided, each emphasizing a different aspect of AgentStudio and its capabilities.\n",
      "\n",
      "The summary, key points, and quotes together provide a comprehensive overview of AgentStudio in a clear and organized manner.\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "Paper: AI Safety - Necessary but insufficient and possibly problematic.pdf\n",
      "\n",
      "=====\n",
      "Response from the Model:\n",
      "Paragraph Summary:\n",
      "This academic paper examines the recent hype around AI safety and argues that while AI safety is important, it may not be enough to ensure that AI benefits society as a whole. The author suggests that the current focus on AI safety is dominated by governments and corporations, and may not adequately address issues like transparency, accountability, and the potential for AI to cause harm to vulnerable populations.\n",
      "\n",
      "Key Points:\n",
      "1. The recent AI safety hype is driven mainly by governments and corporations, not the academic AI community.\n",
      "2. AI safety, as currently discussed, does not necessarily ensure transparency or prevent AI from causing structural harm and disadvantaging certain groups.\n",
      "3. The AI safety debate has already influenced some AI regulations, potentially deprioritizing the protection of individuals from AI-related harms.\n",
      "4. AI labeled as \"safe\" could still be used in harmful ways, such as in discriminatory predictive policing or unfair gig economy practices.\n",
      "\n",
      "Key Quotes:\n",
      "1. \"We posit that AI safety has a nuanced and uneasy relationship with transparency and other allied notions associated with societal good, indicating that it is an insufficient notion if the goal is that of societal good in a broad sense.\"\n",
      "2. \"If AI safety is primed towards addressing visible harm, this begs the question as to where AI safety stands with respect to mandating visibility in AI.\"\n",
      "3. \"AI that is intentionally harmful – such as racist predictive policing algorithms, discriminatory insurance premium determination AI, poor and unsustainable wages determined by gig AI optimized for platform profits – could be fomented under the label of 'safe AI'.\"\n",
      "4. \"To state more provocatively, the contours of the emerging AI safety debate is too risky for humanity in that it may normalize the most potent negative consequence from AI, that of structural harm, and secondly, opaqueness of AI.\"\n",
      "=====\n",
      "Grade:\n",
      "The summary meets the following criteria:\n",
      "\n",
      "1. Understandable by a high school student: The language used in the summary and key points is accessible and not overly technical, making it understandable for a high school student.\n",
      "\n",
      "2. Concise, one-paragraph summary: The summary is contained within a single, concise paragraph that effectively captures the main argument and key points of the academic paper.\n",
      "\n",
      "3. Includes 3-5 clear key points: The summary includes four clear and relevant key points that support the main argument.\n",
      "\n",
      "4. Contains 3-5 relevant quotes: The summary includes four relevant quotes from the academic paper that effectively illustrate and support the key points.\n",
      "\n",
      "Grade: Correct\n",
      "\n",
      "The summary meets all the specified criteria, providing a clear and concise overview of the academic paper that is accessible to a high school student, with well-defined key points and relevant supporting quotes.\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "Paper: Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models.pdf\n",
      "\n",
      "=====\n",
      "Response from the Model:\n",
      "Paragraph Summary:\n",
      "This research paper compares the performance of prompt-engineered Large Multimodal Models (LMMs) and fine-tuned Vision Transformer (ViT) models in two cybersecurity tasks: detecting triggers in images and classifying malware based on visual representations. The study found that while LMMs, like Gemini-pro, can be user-friendly and adaptable, they have limitations when it comes to tasks that require detailed visual understanding. On the other hand, fine-tuned ViT models showed exceptional performance in both tasks, achieving perfect accuracy in trigger detection and high accuracy in malware classification.\n",
      "\n",
      "Key Points:\n",
      "1. Large Multimodal Models (LMMs) can process both text and images but may struggle with tasks requiring detailed visual analysis.\n",
      "2. Vision Transformer (ViT) models, when fine-tuned for specific tasks, demonstrate superior performance in cybersecurity applications.\n",
      "3. Prompt engineering can improve LMM performance to some extent, but it may not be sufficient for complex visual tasks.\n",
      "4. Fine-tuned ViT models outperformed prompt-engineered LMMs in both trigger detection and malware classification tasks.\n",
      "5. The study highlights the potential of fine-tuned ViT models for developing robust AI-driven cybersecurity solutions.\n",
      "\n",
      "Key Quotes:\n",
      "1. \"The success of Large Language Models (LLMs) has led to a parallel rise in the development of Large Multimodal Models (LMMs), such as Gemini-pro, which have begun to transform a variety of applications.\"\n",
      "2. \"Our results highlight a significant divergence in performance, with Gemini-pro falling short in accuracy and reliability when compared to fine-tuned ViT models.\"\n",
      "3. \"The ViT models, on the other hand, demonstrate exceptional accuracy, achieving near-perfect performance on both tasks.\"\n",
      "4. \"The findings suggest that while prompt engineering can enhance the utility of LMMs to some extent, the approach may not always be sufficient for tasks requiring high-level visual comprehension or pattern recognition.\"\n",
      "5. \"The superior performance of ViTs in both visually evident and non-visually evident tasks highlights their potential as a reliable and effective solution for diverse cybersecurity challenges.\"\n",
      "=====\n",
      "Grade:\n",
      "The summary meets the following criteria:\n",
      "\n",
      "1. Understandable by a high school student: The language used in the summary is clear and easy to understand, making it accessible to a high school student.\n",
      "\n",
      "2. Concise, one-paragraph summary: The summary is concise and presented in a single paragraph, providing a quick overview of the research paper.\n",
      "\n",
      "3. Includes 3-5 clear key points: The summary includes five key points that effectively capture the main findings and implications of the study.\n",
      "\n",
      "4. Contains 3-5 relevant quotes: The summary includes five relevant quotes that support the key points and provide additional context.\n",
      "\n",
      "Therefore, I would grade this summary as \"correct\" since it meets all the specified criteria.\n",
      "\n",
      "===================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from anthropic import Anthropic\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# Make sure to visit the ReallyEasy.AI channel at https://www.youtube.com/@reallyeasyai\n",
    "\n",
    "# Initialization with your model name and the client setup.\n",
    "MODEL_NAME = \"claude-3-opus-20240229\"\n",
    "client = Anthropic()  # Assumes ANTHROPIC_API_KEY is set in your environment variables.\n",
    "\n",
    "def reallyeasyai_get_completion(client, model_name, prompt):\n",
    "    \"\"\"Gets model completion for a given prompt.\"\"\"\n",
    "    response = client.messages.create(\n",
    "        model=model_name,\n",
    "        max_tokens=2048,\n",
    "        messages=[{\"role\": 'user', \"content\":  prompt}]\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "def reallyeasyai_read_pdf(file_path):\n",
    "    \"\"\"Reads text from a PDF file and concatenates it into a single string.\"\"\"\n",
    "    reader = PdfReader(file_path)\n",
    "    text = ''.join([page.extract_text() for page in reader.pages if page.extract_text() is not None])\n",
    "    return text\n",
    "\n",
    "def reallyeasyai_generate_summary(client, paper_text):\n",
    "    \"\"\"Generates a summary for the given paper text.\"\"\"\n",
    "    summary_prompt = f\"\"\"\n",
    "    Here is an academic paper: <paper>{paper_text}</paper>\n",
    "    Please summarize this paper at a high school reading level, providing a paragraph summary, 3-5 key points, and 3-5 key quotes.\n",
    "    \"\"\"\n",
    "    return reallyeasyai_get_completion(client, MODEL_NAME, summary_prompt)\n",
    "\n",
    "def reallyeasyai_grade_summary(client, summary):\n",
    "    \"\"\"Grades the given summary based on predefined criteria.\"\"\"\n",
    "    grading_prompt = f\"\"\"\n",
    "    Given this summary, key points, and quotes: {summary}\n",
    "    Assess whether it meets the following criteria:\n",
    "    - Understandable by a high school student.\n",
    "    - Concise, one-paragraph summary.\n",
    "    - Includes 3-5 clear key points.\n",
    "    - Contains 3-5 relevant quotes.\n",
    "    Grade as 'correct' if all criteria are met, otherwise 'needs improvement'.\n",
    "    \"\"\"\n",
    "    return reallyeasyai_get_completion(client, MODEL_NAME, grading_prompt)\n",
    "\n",
    "def reallyeasyai_process_paper(client, file_path):\n",
    "    \"\"\"Processes each paper by generating a summary and grading it.\"\"\"\n",
    "    paper_text = reallyeasyai_read_pdf(file_path)\n",
    "    summary = reallyeasyai_generate_summary(client, paper_text)\n",
    "    grade = reallyeasyai_grade_summary(client, summary)\n",
    "\n",
    "    # Improved output formatting for clarity\n",
    "    print(\"\\n===================================\\n\")\n",
    "    print(f\"Paper: {os.path.basename(file_path)}\\n\")\n",
    "    print(\"=====\\nResponse from the Model:\\n\" + summary)\n",
    "    print(\"=====\\nGrade:\\n\" + grade)\n",
    "    print(\"\\n===================================\\n\")\n",
    "\n",
    "# Specify the path to your papers folder.\n",
    "papers_folder = './papers'\n",
    "\n",
    "for file_name in os.listdir(papers_folder):\n",
    "    if file_name.endswith('.pdf'):\n",
    "        file_path = os.path.join(papers_folder, file_name)\n",
    "        reallyeasyai_process_paper(client, file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
